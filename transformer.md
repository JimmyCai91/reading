#### Reading List 
1. [NLP with Transformers](//C:/Users/caiji/Documents/PDF/Lewis%20Tunstall,%20Leandro%20von%20Werra,%20Thomas%20Wolf%20-%20Natural%20Language%20Processing%20with%20Transformers_%20Building%20Language%20Applications%20with%20Hugging%20Face-O'Reilly%20Media%20(2022).pdf)
2. [2017 Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
3. [2020 An Image Is Worth 16x16 Words](https://arxiv.org/pdf/2010.11929)
4. [2021 Swin Transformer](https://arxiv.org/pdf/2103.14030)


#### Table of Content 
- [Transformer Anatomy](#transformer-anatomy): Encoder, Decoder, MHA, Scaled dot-product Attention, and some of the most prominent architectures

---

#### Transformer Anatomy    
[Table of Content](#table-of-content)  
<div align="center"><img src="../pictures/TransformerAnatomy.png" width=""></div>

---

#### LLM models
- [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up)

